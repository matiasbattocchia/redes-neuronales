{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 5 ‚Äî Introducci√≥n a `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recorrido por las posibilidades de la [librer√≠a](http://scikit-learn.org/stable/user_guide.html)\n",
    "* Familizarizaci√≥n con la [documentaci√≥n](http://scikit-learn.org/stable/modules/classes.html)\n",
    "* Implementaci√≥n de un flujo de trabajo sencillo para regresi√≥n (http://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "\n",
    "https://www.facebook.com/groups/DataScienceArgentina\n",
    "\n",
    "### Utilidad\n",
    "\n",
    "* Aprendizaje supervisado\n",
    "    * Clasificaci√≥n\n",
    "    * Regresi√≥n\n",
    "* Aprendizaje no supervisado\n",
    "* ~~Aprendizaje por refuerzos~~\n",
    "\n",
    "Redes neuronales: hasta perceptr√≥n multicapa.\n",
    "\n",
    "### Extensiones\n",
    "\n",
    "http://scikit-learn.org/stable/related_projects.html\n",
    "\n",
    "* Pandas\n",
    "* M√°s algoritmos\n",
    "* **Automatizaci√≥n** üòÄ\n",
    "* Dominios espec√≠ficos\n",
    "    * Visi√≥n computarizada (im√°genes)\n",
    "    * Procesamiento del lenguaje (texto)\n",
    "    * Medicina, astronom√≠a, geograf√≠a...\n",
    "\n",
    "\n",
    "### Datos\n",
    "\n",
    "`scikit-learn` consume **datos** con forma de matriz o arreglo bidimensional, de dimensi√≥n `(n_muestras, n_atributos)` ‚Äî es como imaginamos normalmente a los datos, dispuestos en una tabla donde las **columnas** son los atributos y hay tantas muestras como **filas**.\n",
    "\n",
    "Convencionalmente en la documentaci√≥n la varible `X` se utiliza para los **atributos** propiamente dichos, y la variable `y` para los **objetivos**. Cuando el objetivo es uno solo, `y` suele tomar la forma de arreglo unidimensional de dimensi√≥n `(n_muestras,)`. \n",
    "\n",
    "### Objetos\n",
    "\n",
    "En `scikit-learn` hay dos tipos fundamentales de objetos:\n",
    "\n",
    "* Los **transformadores**, que implementan los m√©todos\n",
    "    * `fit(X, y)` y\n",
    "    * `transform(X)`,\n",
    "\n",
    "* y los **estimadores**, que implementan\n",
    "    * `fit(X, y)`,\n",
    "    * `predict(X)` y\n",
    "    * dependiendo del estimador, `predict_proba(X)`.\n",
    "\n",
    "### Aprendizaje supervisado\n",
    "    \n",
    "- 1.1. **Generalized Linear Models**\n",
    "- 1.2. Linear and Quadratic Discriminant Analysis\n",
    "- 1.3. Kernel ridge regression\n",
    "- 1.4. **Support Vector Machines**\n",
    "- 1.5. Stochastic Gradient Descent\n",
    "- 1.6. **Nearest Neighbors**\n",
    "- 1.7. Gaussian Processes\n",
    "- 1.8. Cross decomposition\n",
    "- 1.9. **Naive Bayes**\n",
    "- 1.10. **Decision Trees**\n",
    "- 1.11. Ensemble methods\n",
    "- 1.12. Multiclass and multilabel algorithms\n",
    "- 1.13. Feature selection\n",
    "- 1.14. Semi-Supervised\n",
    "- 1.15. Isotonic regression\n",
    "- 1.16. Probability calibration\n",
    "- 1.17. Neural network models (supervised)\n",
    "\n",
    "### Aprendizaje no supervisado\n",
    "\n",
    "- 2.1. Gaussian mixture models\n",
    "- 2.2. Manifold learning\n",
    "- 2.3. **Clustering**\n",
    "- 2.4. Biclustering\n",
    "- 2.5. **Decomposing signals in components** (matrix factorization problems)\n",
    "- 2.6. Covariance estimation\n",
    "- 2.7. Novelty and Outlier Detection\n",
    "- 2.8. Density Estimation\n",
    "- 2.9. Neural network models (unsupervised)\n",
    "\n",
    "### `predict_proba(X)`\n",
    "\n",
    "1.6 http://scikit-learn.org/stable/modules/calibration.html\n",
    "\n",
    "When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo de trabajo\n",
    "\n",
    "![](https://docs.google.com/drawings/d/1HJH4Al7gkcIKOr21w-ciZwAFZad6CsU_YKdeAiHHolA/pub?w=960&h=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos de plantas de iris\n",
    "\n",
    "**Cantidad de instancias**: 150\n",
    "   \t\n",
    "**Atributos** (4)\n",
    "    1. Largo del s√©palo [cm]\n",
    "    2. Ancho del s√©palo [cm]\n",
    "    3. Largo del p√©talo [cm]\n",
    "    4. Ancho del p√©talo [cm]\n",
    "    \n",
    "**Objetivos** (1)\n",
    "    5. clase\n",
    "        Setosa\n",
    "        Versicolour\n",
    "        Virginica\n",
    "\n",
    "**Valores ausentes**: No\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg' alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos \t\t (150, 4)\n",
      "Objetivos \t (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print('Datos', '\\t\\t',   X.shape)\n",
    "print('Objetivos', '\\t', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "\n",
    "Integrando `Pandas` con `scikit-learn` usando el paquete [`sklearn-pandas`](https://github.com/pandas-dev/sklearn-pandas).\n",
    "\n",
    "```\n",
    "# pip install sklearn-pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>pet</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>cat</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>dog</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>dog</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>fish</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>cat</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>dog</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>cat</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>fish</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   children   pet  salary\n",
       "0       4.0   cat      90\n",
       "1       6.0   dog      24\n",
       "2       3.0   dog      44\n",
       "3       3.0  fish      27\n",
       "4       2.0   cat      32\n",
       "5       3.0   dog      59\n",
       "6       5.0   cat      36\n",
       "7       4.0  fish      27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "data = pd.DataFrame({'pet':      ['cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'cat', 'fish'],\n",
    "                     'children': [4., 6, 3, 3, 2, 3, 5, 4],\n",
    "                     'salary':   [90, 24, 44, 27, 32, 59, 36, 27]})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pet_cat</th>\n",
       "      <th>pet_dog</th>\n",
       "      <th>pet_fish</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.876630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.625543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.625543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.459601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.625543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.042572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pet_cat  pet_dog  pet_fish  children\n",
       "0      1.0      0.0       0.0  0.208514\n",
       "1      0.0      1.0       0.0  1.876630\n",
       "2      0.0      1.0       0.0 -0.625543\n",
       "3      0.0      0.0       1.0 -0.625543\n",
       "4      1.0      0.0       0.0 -1.459601\n",
       "5      0.0      1.0       0.0 -0.625543\n",
       "6      1.0      0.0       0.0  1.042572\n",
       "7      0.0      0.0       1.0  0.208514"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    ('pet', sklearn.preprocessing.LabelBinarizer()),\n",
    "    (['children'], sklearn.preprocessing.StandardScaler())\n",
    "], df_out=True)\n",
    "\n",
    "mapper.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "\n",
    "* Cardinalidad\n",
    "* Rango\n",
    "* Desviaci√≥n\n",
    "* Formato\n",
    "    * Booleano\n",
    "    * Num√©ro (separadores)\n",
    "    * Texto\n",
    "        * espacios (*trimming*)\n",
    "        * tildes\n",
    "        * casos (may√∫sculas, min√∫sculas)\n",
    "* **Codificaci√≥n** (UTF-8, etc√©tera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partici√≥n del conjunto de datos\n",
    "\n",
    "* entrenamiento (50%)\n",
    "* validaci√≥n (25%) ‚Äî salvo cross-validation o ausencia de hiperpar√°metros\n",
    "* prueba (25%)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muestreo ‚Äî conjuntos desbalanceados\n",
    "\n",
    "El *entrenamiento* y la *validaci√≥n* de estimadores suele requerir conjuntos de datos **balanceados**; no as√≠ la *prueba* del modelo que debe enfrentar datos reales del problema (**desbalanceados**). `scikit-learn` apenas provee algoritmos de muestro, podemos usar la extensi√≥n [`imbalanced-learn`](http://contrib.scikit-learn.org/imbalanced-learn/index.html) que implementa varios.\n",
    "\n",
    "    # pip install imbalanced-learn\n",
    "\n",
    "`imbalanced-learn` aporta objetos del tipo *muestreador* que implementan los m√©todos `fit(X, y)` y `sample(X)`.\n",
    "\n",
    "**Under-sampling**\n",
    "\n",
    "* ClusterCentroids\n",
    "* RandomUnderSampler\n",
    "\n",
    "**Over-sampling**\n",
    "\n",
    "* SMOTE\n",
    "* RandomOverSampler\n",
    "\n",
    "##### Ejemplo con RandomUnderSampler\n",
    "\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/generated/imblearn.under_sampling.RandomUnderSampler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=200, random_state=10)\n",
    "\n",
    "# Apply the random under-sampling\n",
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_resampled, y_resampled, idx_resampled = rus.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de atributos\n",
    "\n",
    "- 4.1. Pipeline and FeatureUnion: combining estimators\n",
    "- 4.2. Feature extraction\n",
    "- 4.3. Preprocessing data\n",
    "- 4.4. Unsupervised dimensionality reduction\n",
    "- 4.5. Random Projection\n",
    "- 4.6. Kernel Approximation\n",
    "- 4.7. Pairwise metrics, Affinities and Kernels\n",
    "- 4.8. Transforming the prediction target (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n\n",
    "\n",
    "4.2 http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "* Im√°genes\n",
    "* Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaci√≥n\n",
    "\n",
    "4.3.1 http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\n",
    "\n",
    "* Estandarizaci√≥n (StandardScaler) ‚Äî Muy com√∫n; a cada atributo le remueve su valor medio y lo escala dividi√©ndolo por su desviaci√≥n est√°ndar.\n",
    "* Reajuste (MinMaxScaler, MaxAbsScaler)  \n",
    "\n",
    "4.3.2 http://scikit-learn.org/stable/modules/preprocessing.html#normalization\n",
    "\n",
    "* Normalizaci√≥n (Normalizer) ‚Äî Divide vectores por su norma (afecta filas).\n",
    "\n",
    "##### Ejemplo con StandardScaler\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "scaler.transform(X)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputaci√≥n de valores ausentes\n",
    "\n",
    "4.3.5 http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values\n",
    "\n",
    "* Descarte (tirar la muestra)\n",
    "* **Valor m√°s com√∫n**\n",
    "* **Valor promedio**\n",
    "* **Valor medio**\n",
    "* Estimaci√≥n (clasificaci√≥n/regresi√≥n)\n",
    "* Hot-deck (el valor de la muestra m√°s parecida)\n",
    "* NA como otro valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n\n",
    "\n",
    "4.3.6 http://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features\n",
    "\n",
    "De $(X_1, X_2)$ a $(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducci√≥n de dimensionalidad\n",
    "\n",
    "4.4 http://scikit-learn.org/stable/modules/unsupervised_reduction.html\n",
    "\n",
    "* PCA ‚Äî an√°lisis de componentes principales\n",
    "* Proyecciones al azar\n",
    "* Varios estimadores **no supervisados** implementan el m√©todo `transform(X)`\n",
    "\n",
    "##### Ejemplo con PCA\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "\n",
    "pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecci√≥n ‚Äî solo aprendizaje supervisado\n",
    "\n",
    "1.3 http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "* Umbral de varianza\n",
    "* An√°lisis univariado\n",
    "* Usando un estimador\n",
    "* Eliminaci√≥n recursiva (tambi√©n existe la agregaci√≥n recursiva)\n",
    "\n",
    "##### Ejemplo con SelectFromModel\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Load the boston dataset.\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "clf = LassoCV()\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(clf, threshold=0.25)\n",
    "sfm.fit(X, y)\n",
    "sfm.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecci√≥n del modelo\n",
    "\n",
    "- 3.1. Cross-validation: evaluating estimator performance\n",
    "- 3.2. Tuning the hyper-parameters of an estimator\n",
    "- 3.3. Model evaluation: quantifying the quality of predictions\n",
    "- 3.4. Model persistence\n",
    "- 3.5. Validation curves: plotting scores to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluaci√≥n del modelo\n",
    "\n",
    "3.3 http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "Cada estimador implementa un m√©todo llamado `score(X, y)` que devuelve un puntaje del desempe√±o del estimador. El puntaje es calculado usando una m√©trica acorde a la naturaleza del estimador, por ejemplo muchos regresores usan *error cuadr√°tico medio* mientas que muchos clasificadores usan *precisi√≥n*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.50      1.00      0.67         1\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1\n",
    "\n",
    "$F_1 = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = [0, 1, 1, 0, 1, 1]\n",
    "y_pred = [0, 1, 1, 0, 0, 1]\n",
    "f1_score(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cohen's kappa\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cohen's_kappa\n",
    "\n",
    "$\\kappa = \\frac{p_o - p_e}{1 - p_e}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66666666666666674"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [0, 1, 1, 0, 1, 1]\n",
    "y_pred = [0, 1, 1, 0, 0, 1]\n",
    "cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validaci√≥n cruzada\n",
    "\n",
    "3.1 http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Se necesitan dos cosas:\n",
    "- Una estrategia de particionamiento de los datos\n",
    "- Una m√©trica de evaluaci√≥n\n",
    "\n",
    "##### Estrategias\n",
    "\n",
    "* K-fold, stratified k-fold ‚Äî estrategias por defecto pare regresores y clasificadores respectivamente.\n",
    "* Leave one out (LOO)\n",
    "* Leave P out (LPO)\n",
    "* Shuffle & split, stratified shuffle & split\n",
    "\n",
    "![](http://tomaszkacmajor.pl/wp-content/uploads/2016/05/cross-validation.png)\n",
    "\n",
    "##### M√©tricas\n",
    "\n",
    "* De no especificarse ninguna, se usa el m√©todo `score` del estimador.\n",
    "* Las m√©tricas m√°s comunes se pueden pasar como opci√≥n, ver [tabla](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values).\n",
    "* Se pueden armar **puntuadores** a partir de cualquier m√©trica, tanto de la API como definidas por el usuario.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5) \n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizaci√≥n\n",
    "\n",
    "3.2 http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "par√°metros = {\n",
    "    'kernel':('linear', 'rbf'),\n",
    "         'C':[1, 10]\n",
    "}\n",
    "\n",
    "svm = svm.SVC()\n",
    "\n",
    "clf = GridSearchCV(svm, par√°metros)\n",
    "clf.fit(iris.data, iris.target)\n",
    "                            \n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistencia del modelo\n",
    "\n",
    "3.4 http://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "clf = svm.SVC()\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)  \n",
    "\n",
    "import pickle\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)\n",
    "clf2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston price data set\n",
    "\n",
    "**Cantidad de instancias**: 506\n",
    "   \t\n",
    "**Atributos** (13)\n",
    "    1.  CRIM per capita crime rate by town\n",
    "    2.  ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    3.  INDUS proportion of non-retail business acres per town\n",
    "    4.  CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    5.  NOX nitric oxides concentration (parts per 10 million)\n",
    "    6.  RM average number of rooms per dwelling\n",
    "    7.  AGE proportion of owner-occupied units built prior to 1940\n",
    "    8.  DIS weighted distances to five Boston employment centres\n",
    "    9.  RAD index of accessibility to radial highways\n",
    "    10. TAX full-value property-tax rate per 10,000 USD\n",
    "    11. PTRATIO pupil-teacher ratio by town\n",
    "    12. B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    13. LSTAT % lower status of the population\n",
    "\n",
    "**Objetivos** (1)\n",
    "    14. MEDV Median value of owner-occupied homes in 1000‚Äôs USD\n",
    "\n",
    "**Valores ausentes**: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos \t\t (506, 13)\n",
      "Objetivos \t (506,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "X, y   = boston.data, boston.target\n",
    "\n",
    "print('Datos', '\\t\\t',   X.shape)\n",
    "print('Objetivos', '\\t', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Load the boston dataset.\n",
    "boston = load_boston()\n",
    "X, y = boston['data'], boston['target']\n",
    "\n",
    "# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "clf = LassoCV()\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(clf, threshold=0.25)\n",
    "sfm.fit(X, y)\n",
    "n_features = sfm.transform(X).shape[1]\n",
    "\n",
    "# Reset the threshold till the number of features equals two.\n",
    "# Note that the attribute can be set directly instead of repeatedly\n",
    "# fitting the metatransformer.\n",
    "while n_features > 2:\n",
    "    sfm.threshold += 0.1\n",
    "    X_transform = sfm.transform(X)\n",
    "    n_features = X_transform.shape[1]\n",
    "\n",
    "# Plot the selected two features from X.\n",
    "plt.title(\n",
    "    \"Features selected from Boston using SelectFromModel with \"\n",
    "    \"threshold %0.3f.\" % sfm.threshold)\n",
    "feature1 = X_transform[:, 0]\n",
    "feature2 = X_transform[:, 1]\n",
    "plt.plot(feature1, feature2, 'r.')\n",
    "plt.xlabel(\"Feature number 1\")\n",
    "plt.ylabel(\"Feature number 2\")\n",
    "plt.ylim([np.min(feature2), np.max(feature2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "pipe = make_pipeline(Binarizer(), MultinomialNB()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('binarizer', Binarizer(copy=True, threshold=0.0)),\n",
       " ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = dict(reduce_dim__n_components=[2, 5, 10],\n",
    "              clf__C=[0.1, 10, 100])\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de objetivos\n",
    "\n",
    "* Clasificador\n",
    "    * Binario\n",
    "    * **Multi clase**\n",
    "    * **Multi etiqueta**\n",
    "    * Multi clase-etiqueta\n",
    "* Regresor\n",
    "    * Univariado\n",
    "    * Multivariado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TPOT](https://github.com/rhiever/tpot) es una herramienta de aprendizaje autom√°tico automatizado que optimiza el flujo de trabajo.\n",
    "\n",
    "    # pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|‚ñà‚ñà‚ñâ       | 35/120 [00:11<00:41,  2.06pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 13.050775893707737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 50/120 [00:13<00:12,  5.58pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 13.050775893707737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 70/120 [00:29<00:20,  2.49pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 11.119271792901147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 88/120 [00:35<00:12,  2.59pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 11.119271792901147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 11.119271792901147\n",
      "\n",
      "Best pipeline: GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=DEFAULT, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=1.0, GradientBoostingRegressor__min_samples_leaf=11, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.76419689066346"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
